{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from community import community_louvain\n",
    "from typing import Union\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def jaccard_index(dict1: dict, dict2: dict, weight: Union[str, list[str]] = None, denomi_type: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "     - set1, set2: edge/node set or graph with key and attributes\n",
    "     - weight: edge/node feature to calculate Ruzicka similarity\n",
    "     - denomi_type: denominator types. Current implementation only supports union type or 1 (without denominator)\n",
    "    Return:\n",
    "     - Jaccard similarity\n",
    "    \"\"\"\n",
    "    s1 = set(dict1)\n",
    "    s2 = set(dict2)\n",
    "    intersect = s1 & s2\n",
    "    if intersect == 0:\n",
    "        return 0\n",
    "    len_union = len(s1) + len(s2) - len(intersect)\n",
    "\n",
    "    if denomi_type == 'union':\n",
    "        denominator = len_union\n",
    "    else:\n",
    "        denominator = 1\n",
    "    if weight is not None:\n",
    "        intersect_len = weighted_intersection(dict1, dict2, intersect, weight)\n",
    "        return intersect_len / denominator\n",
    "    else:\n",
    "        return len(intersect) / denominator\n",
    "        \n",
    "def weighted_intersection(dict1: dict, dict2: dict, intersect: set, weight: Union[str, list[str]]):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "     - dict1, dict2: dictionary with key and data -- key for intersection, data for similarity\n",
    "     - intersect: list of the keys in intersection\n",
    "    Return:\n",
    "     - Cumulated Ruzicka similarity\n",
    "    \"\"\"\n",
    "    total_len = 0.\n",
    "    for elem in intersect:\n",
    "        if isinstance(weight, list):\n",
    "            total_len += get_vector_sum(dict1.get(elem), dict2.get(elem), weight)\n",
    "        else:\n",
    "            e1 = dict1.get(elem)[weight]\n",
    "            e2 = dict2.get(elem)[weight]\n",
    "            total_len += min(e1,e2) / max(e1,e2)\n",
    "    return total_len\n",
    "\n",
    "def get_vector_sum(e1, e2, weight_list):\n",
    "    mins = 0.\n",
    "    maxs = 0.\n",
    "    for weight in weight_list:\n",
    "        d1 = e1[weight]\n",
    "        d2 = e2[weight]\n",
    "        mins += min(d1, d2)\n",
    "        maxs += max(d1, d2)\n",
    "    return mins / maxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for year_lists.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dict(graph: Union[nx.Graph, nx.DiGraph], main_character: str, level: int) -> dict:\n",
    "    \"\"\"\n",
    "    Extract subgraphs ({1, 1.5}-ego networks) from graph\n",
    "    \"\"\"\n",
    "    if main_character not in graph:\n",
    "        return {}\n",
    "    if level <= 1:\n",
    "        _dict = {(u, v): d for u, v, d in graph.edges(main_character, data = True)}\n",
    "        if isinstance(graph, nx.DiGraph):\n",
    "            _dict.update({(u, v): d for u, v, d in graph.in_edges(main_character, data = True)})\n",
    "    else:\n",
    "        neighbors = list(graph.neighbors(main_character)) + [main_character]\n",
    "        subgraph = graph.subgraph(neighbors)\n",
    "        _dict = {(u,v): d for u,v,d in subgraph.edges(data = True)}\n",
    "    return _dict\n",
    "        \n",
    "\n",
    "def get_comparison(graph1: Union[nx.Graph, nx.DiGraph], graph2: Union[nx.Graph, nx.DiGraph],\n",
    "                   denomi_type: str, level = 1, weight: Union[str, list[str]] = 'weight') -> dict:\n",
    "    \"\"\"\n",
    "    Calculate distances of each characters in two graphs\n",
    "    \"\"\"\n",
    "    nodes = set(graph1.nodes) | set(graph2.nodes)\n",
    "    _dict = {}\n",
    "    for node in nodes:\n",
    "        g1 = get_dict(graph1, node, level)\n",
    "        g2 = get_dict(graph2, node, level)\n",
    "        len_union = len(set(g1) | set(g2))\n",
    "        sim = jaccard_index(g1, g2, weight, denomi_type)\n",
    "        dist = 1 - sim if denomi_type == 'union' else len_union - sim\n",
    "        _dict[node] = dist\n",
    "    return _dict\n",
    "\n",
    "def convert_yearlist_to_dissim(_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Convert year_lists.json format into the dissim.json format\n",
    "    \"\"\"\n",
    "    dissims = defaultdict(list)\n",
    "    _max = -1e9\n",
    "    _min = 1e9\n",
    "    for item in _dict:\n",
    "        dists = item['distance']\n",
    "        _from = item['from']\n",
    "        _to = item['to']\n",
    "        for node, dist in dists.items():\n",
    "            dissims[node].append({\n",
    "                \"distance\": dist,\n",
    "                \"from\": _from,\n",
    "                \"to\": _to\n",
    "            })\n",
    "            _max = max(_max, dist)\n",
    "            _min = min(_min, dist)\n",
    "    dissims[\"metadata\"].append({\n",
    "        \"max\": _max,\n",
    "        \"min\": _min\n",
    "    })\n",
    "    return dict(dissims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisPub data Processing\n",
    "\n",
    "Download VisPub data from https://vispubdata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/VisPub/vispubdata.csv\") #Load vispub data\n",
    "yearly_graphs = {}\n",
    "# Iterate through the data and create graphs\n",
    "for index, row in data.iterrows():\n",
    "    year = row[\"Year\"]\n",
    "    if year not in yearly_graphs:\n",
    "        yearly_graphs[year] = nx.Graph()\n",
    "    authors = row[\"AuthorNames-Deduped\"].split(';')\n",
    "    graph = yearly_graphs[year]\n",
    "    for i in range(len(authors)):\n",
    "        for j in range(i + 1, len(authors)):\n",
    "            if not graph.has_edge(authors[i], authors[j]):\n",
    "                graph.add_edge(authors[i], authors[j], weight=0)\n",
    "            graph[authors[i]][authors[j]]['weight'] += 1\n",
    "sorted_yearly_graphs = {year: yearly_graphs[year] for year in sorted(yearly_graphs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Generate ego/year_lists.json and ego/dissim.json --> set level 1\n",
    "\"\"\"\n",
    "lbd = min(sorted_yearly_graphs.keys())\n",
    "ubd = max(sorted_yearly_graphs.keys())\n",
    "\n",
    "is_ego = True\n",
    "\n",
    "level = 2 - is_ego\n",
    "save_results = True\n",
    "\n",
    "year_lists = []\n",
    "for t1 in tqdm(range(lbd, ubd)):\n",
    "    for t2 in range(t1 + 1, ubd + 1):\n",
    "        _dict = get_comparison(sorted_yearly_graphs[t1], sorted_yearly_graphs[t2], denomi_type = None, level = level, weight = 'weight')\n",
    "        year_lists.append({\n",
    "            'distance': _dict,\n",
    "            'from': t1,\n",
    "            'to': t2\n",
    "        })\n",
    "\n",
    "if save_results:\n",
    "    if not os.path.exists('ego'):\n",
    "        os.makedirs('ego')\n",
    "    json.dump(year_lists, open('ego/year_lists.json', 'w'))\n",
    "    json.dump(convert_yearlist_to_dissim(year_lists), open('ego/year_lists.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Generate neighbor/year_lists.json and neighbor/dissim.json --> set level > 1\n",
    "\"\"\"\n",
    "lbd = min(sorted_yearly_graphs.keys())\n",
    "ubd = max(sorted_yearly_graphs.keys())\n",
    "\n",
    "is_ego = False\n",
    "\n",
    "level = 2 - is_ego\n",
    "save_results = True\n",
    "\n",
    "year_lists = []\n",
    "for t1 in tqdm(range(lbd, ubd)):\n",
    "    for t2 in range(t1 + 1, ubd + 1):\n",
    "        _dict = get_comparison(sorted_yearly_graphs[t1], sorted_yearly_graphs[t2], denomi_type = None, level = level, weight = 'weight')\n",
    "        year_lists.append({\n",
    "            'distance': _dict,\n",
    "            'from': t1,\n",
    "            'to': t2\n",
    "        })\n",
    "\n",
    "if save_results:\n",
    "    if not os.path.exists('neighbor'):\n",
    "        os.makedirs('neighbor')\n",
    "    json.dump(year_lists, open('neighbor/year_lists.json', 'w'))\n",
    "    json.dump(convert_yearlist_to_dissim(year_lists), open('neighbor/year_lists.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate comm_lists.json and partition_lists.json\n",
    "\"\"\"\n",
    "def get_partition_subgraph(graph, partition_dict, target_node):\n",
    "    comm = partition_dict[target_node]\n",
    "    subgraph_nodes = [k for k, v in partition_dict.items() if v == comm]\n",
    "    return graph.subgraph(subgraph_nodes)\n",
    "\n",
    "save_results = True\n",
    "\n",
    "partition_list = {}\n",
    "comm_list = {}\n",
    "\n",
    "for t, graph in sorted_yearly_graphs.items(): \n",
    "    G = graph.to_undirected()\n",
    "    partition = community_louvain.best_partition(G)\n",
    "    comm = defaultdict(list) # dictionary which has list-type values for detailed implementation, please check defaultdict\n",
    "    for k, v in partition.items():\n",
    "        comm[v].append(k)\n",
    "    partition_list[t] = {k: str(t) + \"-\" + str(v) for k,v in partition.items()}\n",
    "    comm_list[t] = {str(t) + \"-\" + str(k): v for k, v in dict(comm).items()} # change defaultdict to python original dictionary type\n",
    "if save_results:\n",
    "    json.dump(partition_list, open('partition_lists.json', 'w'))\n",
    "    json.dump(comm_list, open('comm_lists.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
